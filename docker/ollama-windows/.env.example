# =============================================================================
# Ollama on Windows Configuration
# =============================================================================

# Timezone
TZ=America/New_York

# =============================================================================
# Performance Configuration
# =============================================================================

# Number of parallel requests
# Increase if you have multiple agents or concurrent users
OLLAMA_NUM_PARALLEL=2

# Maximum loaded models
# Keep at 1 for most GPU setups to avoid memory issues
OLLAMA_MAX_LOADED_MODELS=1

# Model keep-alive duration
# Prevents models from unloading during overnight runs
# Format: 5m, 24h, 0 (never unload)
OLLAMA_KEEP_ALIVE=24h

# Maximum request queue size
OLLAMA_MAX_QUEUE=512

# =============================================================================
# Recommended Models (pull after deployment)
# =============================================================================
# Pull these models in Docker container:
# docker exec -it ollama-windows ollama pull llama3.2:3b
# docker exec -it ollama-windows ollama pull qwen2.5-coder:7b
# docker exec -it ollama-windows ollama pull deepseek-coder-v2:16b
